








import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt





df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
df.head()








print(f"Total Employees: {df.shape[0]}")
print(df["Attrition"].value_counts())





df.groupby('Attrition')['Age'].mean()





def ar(x):
    total = x["Department"].value_counts()
    left = x[x["Attrition"] == "Yes"]["Department"].value_counts()
    rate = (left / total) * 100
    return rate.round(2)

attrition_rate = ar(df)
print(attrition_rate)







df_corr = df.copy()
df_corr["Attrition"] = df_corr["Attrition"].map({"Yes": 1, "No": 0})

# Correlation between DistanceFromHome and Attrition
corr = df_corr["DistanceFromHome"].corr(df_corr["Attrition"])
print("Correlation:", round(corr, 2))






pd.crosstab(df["OverTime"], df["Attrition"], normalize="index") * 100





df.groupby("Attrition")["MonthlyIncome"].mean()





print("Missing Values in Each Column:")
print(df.isnull().sum().sort_values(ascending=False))

missing = df.isnull().sum()
missing = missing[missing > 0]

if missing.empty:
    print("There are no missing values in the dataset.")
else:
    print("Features with missing values:")
    print(missing.sort_values(ascending=False))








sns.countplot(x="Attrition", data=df)





sns.boxplot(x="Attrition", y="MonthlyIncome", data=df)


plt.figure(figsize=(8, 8))
sns.histplot(data=df, x="Age", hue="Attrition", bins=30, kde=True)





pd.crosstab(df["Department"], df["Attrition"]).plot(kind="bar", stacked=True)





sns.countplot(x="JobSatisfaction", hue="Attrition", data=df)





plt.figure(figsize=(12, 10))
sns.heatmap(df.select_dtypes(include="number").corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap (Numeric Variables Only)")








# For ML
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler






df_encoded = df.copy()

# Drop columns not useful
df_encoded.drop(["EmployeeNumber", "Over18", "StandardHours", "EmployeeCount"], axis=1, inplace=True)

# Encode binary variables
le = LabelEncoder()
df_encoded["Attrition"] = le.fit_transform(df_encoded["Attrition"])  # Yes=1, No=0

# OneHot encode categorical features
df_encoded = pd.get_dummies(df_encoded, drop_first=True)






X = df_encoded.drop("Attrition", axis=1)
y = df_encoded["Attrition"]





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)








l_model = LogisticRegression(max_iter=1000)
l_model.fit(X_train, y_train)
l_pred = l_model.predict(X_test)





rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)








def evaluate(y_test, y_pred, model_name):
    print(f"\n--- {model_name} ---")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))





evaluate(y_test, l_pred, "Logistic Regression")
evaluate(y_test, rf_pred, "Random Forest")






l_prob = l_model.predict_proba(X_test)[:,1]
rf_prob = rf_model.predict_proba(X_test)[:,1]

l_fpr, l_tpr, _ = roc_curve(y_test, l_prob)
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_prob)

plt.figure(figsize=(8,6))
plt.plot(l_fpr, l_tpr, label='Logistic Regression')
plt.plot(rf_fpr, rf_tpr, label='Random Forest')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()




